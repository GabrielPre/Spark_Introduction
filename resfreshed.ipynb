{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project NoSQL: Apache Spark\n",
    "**By Stanislas KIESGEN DE RICHTER and Gabriel PRECIGOUT.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1.Introduction: short introduction on spark and on the project  \n",
    "2.Installation: installation procedure of Spark and PySpark  \n",
    "3.Querries from scratch: we will then do simple querries from scratch to create alter and find informations on a table  \n",
    "4.Dataset Manipulation: querries and pyspark function on an imported dataset  \n",
    "5.Comparison with other technologies: Specs and efficiency comparaison with other technologies\n",
    "## Part 1: Introduction\n",
    "In this project we will present an introduction to Spark, and more precisly Spark SQL and PySpark.\n",
    "Apache Spark is a cluster computing framework containing multiple tools revovling around Spark Core.\n",
    "\n",
    "For the history\n",
    "\n",
    "Spark was at first begun by Matei Zaharia at UC Berkeley's AMPLab in 2009, and publicly released as an open source project in 2010 under a BSD license.\n",
    "\n",
    "\n",
    "Three years later, the project was given to the Apache Software Foundation and switched its license to Apache 2.0. In February 2014, it became a Top-Level Apache Project.\n",
    "\n",
    "In November 2014, Spark founder M. Zaharia's company Databricks set a new world record in large scale sorting using Spark. Making it well know for its efficiency, even on massive data amounts.\n",
    "\n",
    "Spark had more than 1000 contributors in 2015, making it one of the most dynamic projects in the Apache Software Foundation and one of the most active open source big data projects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Install Java 8 or check its version\n",
    "\n",
    "Before installing Apache Spark we need to check if Java and JDK 8 are installed using the command prompt.\n",
    "\n",
    "Let's open it and check our Java versions:\n",
    "```\n",
    "C:\\Users\\gabriel>java -version\n",
    "openjdk version \"1.8.0_272\"\n",
    "OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_272-b10)\n",
    "OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.272-b10, mixed mode)\n",
    "```\n",
    "\n",
    "### 2.2. Install Python or check its version\n",
    "\n",
    "We also need to install python before we can use Apache Spark, so let's check our version:\n",
    "```\n",
    "C:\\Users\\gabriel>python\n",
    "Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32\n",
    "```\n",
    "\n",
    "### 2.3. Downloading Apache Spark\n",
    "\n",
    "We need to go the following website: https://spark.apache.org/downloads.html.\n",
    "Once we dowloaded it, we need to create a folder wherever you want that we'll call Spark and extract the compressed file in there.\n",
    "When the Spark File is full, we should open the conf folder and rename the file log4j.properties.template to log4j.properties and open it with wordpad for example.\n",
    "\n",
    "Inside the log4j.properties, we'll find the following line:\n",
    "```\n",
    "# Set everything to be logged to the console\n",
    "log4j.rootCategory=INFO, console\n",
    "```\n",
    "We will change it to:\n",
    "```\n",
    "# Set everything to be logged to the console\n",
    "log4j.rootCategory=ERROR, console\n",
    "```\n",
    "This change will remove all the logs that gets prints off when we'll run commands with Spark.\n",
    "Save the file and close it.\n",
    "\n",
    "### 2.4. winutils\n",
    "\n",
    "If you're not on windows you can skip this step, otherwise it's recommended.\n",
    "You need to download winutils, you can find it here (https://github.com/steveloughran/winutils)\n",
    "This is a windows binary, Hadoop requires native libraries on Windows to work properly -that includes accessing the file:// filesystem, where Hadoop uses some Windows APIs to implement posix-like file access permissions.\n",
    "\n",
    "This is implemented in hadoop.dll and winutils.exe.\n",
    "\n",
    "Now we need to create a winutil folder at the root of our Spark folder and we can call this new folder \"winutils\", inside it we'll create another folder called \"bin\" and copy our winutils.exe here.\n",
    "\n",
    "Open the command prompt (CMD) and type the following commands:\n",
    "```\n",
    "C:\\Users\\gabki>cd C:\\Program Files (x86)\\winutils\\bin\n",
    "\n",
    "C:\\Program Files (x86)\\winutils\\bin>dir\n",
    " Le volume dans le lecteur C s’appelle Windows-SSD\n",
    " Le numéro de série du volume est D801-AF53\n",
    "\n",
    " Répertoire de C:\\Program Files (x86)\\winutils\\bin\n",
    "\n",
    "14/12/2020  22:40    <DIR>          .\n",
    "14/12/2020  22:40    <DIR>          ..\n",
    "14/12/2020  22:27           108 032 winutils.exe\n",
    "               1 fichier(s)          108 032 octets\n",
    "               2 Rép(s)  16 753 942 528 octets libres\n",
    "C:\\Program Files (x86)\\winutils\\bin>mkdir \\tmp\\hive\n",
    "C:\\Program Files (x86)\\winutils\\bin>winutils.exe ls \\tmp\\hive\n",
    "d--------- 1 DESKTOP-2KDT29C\\gabki DESKTOP-2KDT29C\\gabki 0 Dec 14 2020 \\tmp\\hive\n",
    "C:\\Program Files (x86)\\winutils\\bin>dir \\tmp\\hive\n",
    " Le volume dans le lecteur C s’appelle Windows-SSD\n",
    " Le numéro de série du volume est D801-AF53\n",
    "\n",
    " Répertoire de C:\\tmp\\hive\n",
    "\n",
    "14/12/2020  22:57    <DIR>          .\n",
    "14/12/2020  22:57    <DIR>          ..\n",
    "               0 fichier(s)                0 octets\n",
    "               2 Rép(s)  17 155 751 936 octets libres\n",
    "C:\\Program Files (x86)\\winutils\\bin>set path=%path%;\"C:\\Program Files (x86)\\winutils\\bin\"\n",
    "C:\\Program Files (x86)\\winutils\\bin>winutils.exe ls \\tmp\\hive\n",
    "drwxrwxrwx 1 DESKTOP-2KDT29C\\gabki DESKTOP-2KDT29C\\gabki 0 Dec 14 2020 \\tmp\\hive\n",
    "```\n",
    "\n",
    "### 2.5. Definition of the environment variables\n",
    "\n",
    "We need to define the environment variables and we'll be ready to use Spark!\n",
    "\n",
    "Let's define the SPARK_HOME variable with the path to our Spark folder;\n",
    "JAVA_HOME with the path to the java JDK\n",
    "HADOOP_HOME with the path to the winutils\n",
    "\n",
    "### 2.6. Check your installation\n",
    "\n",
    "Now that everything is setup you can test spark to see if it runs!\n",
    "```\n",
    "C:\\Users\\gabriel>cd c:\\Spark\\bin\n",
    "c:\\Spark\\bin>pyspark\n",
    "Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020 15:52:53)\n",
    "SparkSession available as 'spark'.\n",
    "```\n",
    "We can also test the following programs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Querries from scratch\n",
    "\n",
    "First we need to initialize our SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "\n",
    "sc = SparkContext().getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Creating a Dataframe\n",
    "\n",
    "Let's create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+\n",
      "|studentID|studentName|      city|\n",
      "+---------+-----------+----------+\n",
      "|        1|    Gabriel|     Paris|\n",
      "|        2|  Stanislas|Montpelier|\n",
      "+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from array import array\n",
    "data = [(1, \"Gabriel\", \"Paris\"),(2, \"Stanislas\", \"Montpelier\")]\n",
    "#dataSchema = StructType(array(\n",
    "#    StructField(\"studentID\", IntegerType(), False),\n",
    "#    StructField(\"studentName\", StringType(), True),\n",
    "#    StructField(\"city\", StringType(), True)))\n",
    "rdd = sc.parallelize(data)\n",
    "students = rdd.map(lambda x: Row(studentID=int(x[0]), studentName=x[1], city=x[2]))\n",
    "table = sqlContext.createDataFrame(students) #, dataSchema\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Show the datatype\n",
    "We can see the datatypes of each column with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- studentID: long (nullable = true)\n",
      " |-- studentName: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Add a column\n",
    "You can add a column with the function withColumns which edit an existing column or creates it if it doesn't exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+-----------+\n",
      "|studentID|studentName|      city|pocketMoney|\n",
      "+---------+-----------+----------+-----------+\n",
      "|        1|    Gabriel|     Paris|         50|\n",
      "|        2|  Stanislas|Montpelier|         50|\n",
      "+---------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "table = table.withColumn(\"pocketMoney\", lit(50))\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Drop a column\n",
    "You can drop a column from the dataframe using the drop operation, we will drop our newly created column 'pocketMoney'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+\n",
      "|studentID|studentName|      city|\n",
      "+---------+-----------+----------+\n",
      "|        1|    Gabriel|     Paris|\n",
      "|        2|  Stanislas|Montpelier|\n",
      "+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = table.drop(\"pocketMoney\")\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Count the number of rows and colums\n",
    "You can count the number of rows using the count method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of columns and display their names using the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, ['studentID', 'studentName', 'city'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table.columns), table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Show a summary of the dataset\n",
    "\n",
    "This will display the mean, standard deviation, min, max, count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------+----------+\n",
      "|summary|         studentID|studentName|      city|\n",
      "+-------+------------------+-----------+----------+\n",
      "|  count|                 2|          2|         2|\n",
      "|   mean|               1.5|       null|      null|\n",
      "| stddev|0.7071067811865476|       null|      null|\n",
      "|    min|                 1|    Gabriel|Montpelier|\n",
      "|    max|                 2|  Stanislas|     Paris|\n",
      "+-------+------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Update data in a row\n",
    "We will use the filter method to select the row we want to modify and specify the column of the value to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------+\n",
      "|studentID|studentName|     city|\n",
      "+---------+-----------+---------+\n",
      "|        1|    Gabriel|    Paris|\n",
      "|        2|  Stanislas|Villejuif|\n",
      "+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "table = table.withColumn(\"city\",\n",
    "                            when(table[\"studentID\"] == 2,\"Villejuif\").otherwise(table[\"city\"]))\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. SQL Context\n",
    "\n",
    "We can also do some query operation using the SQL context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     city|\n",
      "+---------+\n",
      "|Villejuif|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.createOrReplaceTempView(\"table\")\n",
    "sqlContext.sql(\"SELECT city FROM table where studentID>=2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dataset Manipulation\n",
    "### 4.1. We first need to import a dataset\n",
    "Spark as an automatic formater for csv , json, and text we will use a csv with countries characteristics here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.option(\"header\",True).option(\"delimiter\",\";\").option(\"inferSchema\",True).csv(\"factbook.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Area(sq km): integer (nullable = true)\n",
      " |-- Birth rate(births/1000 population): double (nullable = true)\n",
      " |-- Current account balance: long (nullable = true)\n",
      " |-- Death rate(deaths/1000 population): double (nullable = true)\n",
      " |-- Debt - external: long (nullable = true)\n",
      " |-- Electricity - consumption(kWh): long (nullable = true)\n",
      " |-- Electricity - production(kWh): long (nullable = true)\n",
      " |-- Exports: long (nullable = true)\n",
      " |-- GDP: long (nullable = true)\n",
      " |-- GDP - per capita: integer (nullable = true)\n",
      " |-- GDP - real growth rate(%): double (nullable = true)\n",
      " |-- HIV/AIDS - adult prevalence rate(%): double (nullable = true)\n",
      " |-- HIV/AIDS - deaths: integer (nullable = true)\n",
      " |-- HIV/AIDS - people living with HIV/AIDS: integer (nullable = true)\n",
      " |-- Highways(km): integer (nullable = true)\n",
      " |-- Imports: long (nullable = true)\n",
      " |-- Industrial production growth rate(%): double (nullable = true)\n",
      " |-- Infant mortality rate(deaths/1000 live births): double (nullable = true)\n",
      " |-- Inflation rate (consumer prices)(%): double (nullable = true)\n",
      " |-- Internet hosts: integer (nullable = true)\n",
      " |-- Internet users: integer (nullable = true)\n",
      " |-- Investment (gross fixed)(% of GDP): double (nullable = true)\n",
      " |-- Labor force: integer (nullable = true)\n",
      " |-- Life expectancy at birth(years): double (nullable = true)\n",
      " |-- Military expenditures - dollar figure: long (nullable = true)\n",
      " |-- Military expenditures - percent of GDP(%): double (nullable = true)\n",
      " |-- Natural gas - consumption(cu m): long (nullable = true)\n",
      " |-- Natural gas - exports(cu m): long (nullable = true)\n",
      " |-- Natural gas - imports(cu m): long (nullable = true)\n",
      " |-- Natural gas - production(cu m): long (nullable = true)\n",
      " |-- Natural gas - proved reserves(cu m): long (nullable = true)\n",
      " |-- Oil - consumption(bbl/day): integer (nullable = true)\n",
      " |-- Oil - exports(bbl/day): integer (nullable = true)\n",
      " |-- Oil - imports(bbl/day): integer (nullable = true)\n",
      " |-- Oil - production(bbl/day): integer (nullable = true)\n",
      " |-- Oil - proved reserves(bbl): long (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- Public debt(% of GDP): double (nullable = true)\n",
      " |-- Railways(km): integer (nullable = true)\n",
      " |-- Reserves of foreign exchange & gold: long (nullable = true)\n",
      " |-- Telephones - main lines in use: integer (nullable = true)\n",
      " |-- Telephones - mobile cellular: integer (nullable = true)\n",
      " |-- Total fertility rate(children born/woman): double (nullable = true)\n",
      " |-- Unemployment rate(%): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------------+\n",
      "|             Country|Birth rate(births/1000 population)|\n",
      "+--------------------+----------------------------------+\n",
      "|         Afghanistan|                             47.02|\n",
      "|            Akrotiri|                              null|\n",
      "|             Albania|                             15.08|\n",
      "|             Algeria|                             17.13|\n",
      "|      American Samoa|                             23.13|\n",
      "|             Andorra|                               9.0|\n",
      "|              Angola|                             44.64|\n",
      "|            Anguilla|                             14.26|\n",
      "|          Antarctica|                              null|\n",
      "| Antigua and Barbuda|                             17.26|\n",
      "|           Argentina|                              16.9|\n",
      "|             Armenia|                             11.76|\n",
      "|               Aruba|                             11.26|\n",
      "|Ashmore and Carti...|                              null|\n",
      "|           Australia|                             12.26|\n",
      "|             Austria|                              8.81|\n",
      "|          Azerbaijan|                              20.4|\n",
      "|         Bahamas The|                             17.87|\n",
      "|             Bahrain|                              18.1|\n",
      "|        Baker Island|                              null|\n",
      "+--------------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"Country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the import worked properly and we now have a dataset on which we can do querries. \n",
    "\n",
    "We will try to do some simple as well as more complex querries to see how efficient spark is.\n",
    "\n",
    "### 4.2. Querries examples\n",
    "\n",
    "#### 4.2.1. Where clause example\n",
    "Using dsl querries we can select the countries where the deat rate is superior to 20. Meaning the countries where more than 20 people our of 1000 dies a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------------+\n",
      "|             Country|Death rate(deaths/1000 population)|\n",
      "+--------------------+----------------------------------+\n",
      "|         Afghanistan|                             20.75|\n",
      "|              Angola|                              25.9|\n",
      "|            Botswana|                             29.36|\n",
      "|Central African R...|                             20.27|\n",
      "|             Lesotho|                             25.03|\n",
      "|              Malawi|                             23.39|\n",
      "|          Mozambique|                             20.99|\n",
      "|               Niger|                             21.33|\n",
      "|        Sierra Leone|                             20.61|\n",
      "|        South Africa|                             21.32|\n",
      "|           Swaziland|                             25.26|\n",
      "|              Zambia|                             20.23|\n",
      "|            Zimbabwe|                             24.66|\n",
      "+--------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dataset.select(\"Country\",\"Death rate(deaths/1000 population)\").where(col(\"Death rate(deaths/1000 population)\")>20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Max Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|max(Area(sq km))|\n",
      "+----------------+\n",
      "|        17075200|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.agg({\"Area(sq km)\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Order By example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+\n",
      "|          Country|          GDP|\n",
      "+-----------------+-------------+\n",
      "|             Iraq|  89800000000|\n",
      "|             Chad|  15660000000|\n",
      "|          Liberia|   2903000000|\n",
      "|Equatorial Guinea|   1270000000|\n",
      "|        Venezuela| 145200000000|\n",
      "|            Macau|   9100000000|\n",
      "|          Ukraine| 299100000000|\n",
      "|           Angola|  23170000000|\n",
      "|         Ethiopia|  54890000000|\n",
      "|    Liechtenstein|    825000000|\n",
      "|         Mongolia|   5332000000|\n",
      "|       Tajikistan|   7950000000|\n",
      "|          Uruguay|  49270000000|\n",
      "|    Faroe Islands|   1000000000|\n",
      "|       Azerbaijan|  30010000000|\n",
      "|          Georgia|  14450000000|\n",
      "|       Kazakhstan| 118400000000|\n",
      "|            China|7262000000000|\n",
      "|          Armenia|  13650000000|\n",
      "|            Qatar|  19490000000|\n",
      "+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc,desc\n",
    "dataset.select(\"Country\",\"GDP\").orderBy(col(\"GDP - real growth rate(%)\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the Highest GDP using the collect method in this command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|           GDP|      Country|\n",
      "+--------------+-------------+\n",
      "|11750000000000|United States|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"GDP\",\"Country\").orderBy(col(\"GDP\").desc()).limit(1).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4. Nested querry example\n",
    "We can find the country with the second highest gdp using a nested querry like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|        Country|           GDP|\n",
      "+---------------+--------------+\n",
      "|#European Union|11650000000000|\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"Country\",\"GDP\").where(col(\"GDP\") <   \\\n",
    "                                      ( dataset.select(\"GDP\").orderBy(col(\"GDP\").desc()).limit(1).collect()[0][\"GDP\"] ) \\\n",
    "                                      ).orderBy(col(\"GDP\").desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset the EU is considered a country and as the second highest GPD behind the USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.3. Other PySpark features\n",
    "Using pyspark allows to use python fucntions to alter or analyse our dataset, we will see some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1. Describe \n",
    "The describe function allows to process the count, mean, standard deviation, min & max of the columns selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------------+\n",
      "|summary|                 GDP|      Area(sq km)|      Railways(km)|\n",
      "+-------+--------------------+-----------------+------------------+\n",
      "|  count|                 230|              263|               134|\n",
      "|   mean|2.925613713043478...|584987.4866920152|10217.246268656716|\n",
      "| stddev|1.252331334754941...|1881415.546777828|29857.440213148762|\n",
      "|    min|             1500000|                0|                 6|\n",
      "|    max|      11750000000000|         17075200|            228464|\n",
      "+-------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.describe(\"GDP\",\"Area(sq km)\",\"Railways(km)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2. Binarizer\n",
    "We can apply a threshold to a column to replace values by 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 15.000000\n",
      "+--------------------+----------------------------------+------------+\n",
      "|             Country|Birth rate(births/1000 population)|binarized_br|\n",
      "+--------------------+----------------------------------+------------+\n",
      "|         Afghanistan|                             47.02|         1.0|\n",
      "|            Akrotiri|                              null|        null|\n",
      "|             Albania|                             15.08|         1.0|\n",
      "|             Algeria|                             17.13|         1.0|\n",
      "|      American Samoa|                             23.13|         1.0|\n",
      "|             Andorra|                               9.0|         0.0|\n",
      "|              Angola|                             44.64|         1.0|\n",
      "|            Anguilla|                             14.26|         0.0|\n",
      "|          Antarctica|                              null|        null|\n",
      "| Antigua and Barbuda|                             17.26|         1.0|\n",
      "|           Argentina|                              16.9|         1.0|\n",
      "|             Armenia|                             11.76|         0.0|\n",
      "|               Aruba|                             11.26|         0.0|\n",
      "|Ashmore and Carti...|                              null|        null|\n",
      "|           Australia|                             12.26|         0.0|\n",
      "|             Austria|                              8.81|         0.0|\n",
      "|          Azerbaijan|                              20.4|         1.0|\n",
      "|         Bahamas The|                             17.87|         1.0|\n",
      "|             Bahrain|                              18.1|         1.0|\n",
      "|        Baker Island|                              null|        null|\n",
      "+--------------------+----------------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "\n",
    "binarizer = Binarizer(threshold=15, inputCol=\"Birth rate(births/1000 population)\", outputCol=\"binarized_br\")\n",
    "\n",
    "bdataset = binarizer.transform(dataset)\n",
    "\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "bdataset.select(\"Country\",\"Birth rate(births/1000 population)\",\"binarized_br\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only wanted to show that PySpark could do way more than simple SQL and we will not present every feature here, a non-exhaustive list can be found here: https://spark.apache.org/docs/2.2.0/ml-features.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparison with other technologies\n",
    "| Name                | Cassandra         | MariaDB         | MongoDB        | Spark           |\n",
    "| ------------------- | ----------------- | --------------- | -------------- | --------------- |\n",
    "| Description         | Wide-column store based on ideas of BigTable and DynamoDB | MySQL application compatible open source RDBMS, enhanced with high availability, security, interoperability and performance capabilities. With MariaDB ColumnStore a column-oriented storage engine is available too | One of the most popular document stores available both as a fully managed cloud service and for deployment on self-managed infrastructure | Spark SQL is a component on top of 'Spark Core' for structured data processing |\n",
    "| Primary DB model    | Wide column store | Relational DBMS | Document store | Relational DBMS |\n",
    "| Secondary DB models |                   | Document store  | Search engine,Graph DBMS |                 |\n",
    "| Initial release     | 2008 | 2009 | 2009 | 2014 |\n",
    "| License             | Open Source | Open Source | Open Source | Open Source |\n",
    "| Implementation language | Java | C and C++ | C++ | Scala |\n",
    "| Supported programming languages | C#, C++, Clojure, Erlang, Go, Haskell, Java, JavaScript, Pearl, PHP, Python, Ruby, Scala | Ada, C, C#, C++, D, Eiffel, Erlang, Go, Haskell, Java, JavaScript (Node.js), Objective-C, Ocaml, Pearl, PHP, Python, Ruby, Scheme, Tcl | ActionScript, C, C#, C++, Clojure, ColdFusion, D, Dart, Delphi, Erlang, Go, Groovy, Haskell,  Java, JavaScript, Lisp, Lua, Matlab, Perl, PHP, PowerShell, Prolog, Python, R, Ruby, Rust, Scala, Smalltalk, Swift | Java, Python, R, Scala |\n",
    "| Server-side scripts | no | yes | JavaScript | no |\n",
    "| partitioning methods | Sharding | Horizontal partitioning, sharding with Spider storage engine or Galera cluster | Sharding | Yes, utilizing Spark Core |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "From what we saw Spark support sql and dsl querries. We can do everything SQL does and even more using pySpark or other modules, allowing to use others function under a python environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
