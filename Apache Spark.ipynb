{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project NoSQL: Apache Spark\n",
    "Made by Stanislas KIESGEN DE RICHTER and Gabriel PRECIGOUT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Install Java 8 or check its version\n",
    "\n",
    "Before installing Apache Spark we need to check if Java and JDK 8 are installed using the command prompt.\n",
    "\n",
    "Let's open it and check our Java versions:\n",
    "```\n",
    "C:\\Users\\gabriel>java -version\n",
    "openjdk version \"1.8.0_272\"\n",
    "OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_272-b10)\n",
    "OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.272-b10, mixed mode)\n",
    "```\n",
    "\n",
    "### 2.2. Install Python or check its version\n",
    "\n",
    "We also need to install python before we can use Apache Spark, so let's check our version:\n",
    "```\n",
    "C:\\Users\\gabriel>python\n",
    "Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32\n",
    "```\n",
    "\n",
    "### 2.3. Downloading Apache Spark\n",
    "\n",
    "We need to go the following website: https://spark.apache.org/downloads.html.\n",
    "Once we dowloaded it, we need to create a folder wherever you want that we'll call Spark and extract the compressed file in there.\n",
    "When the Spark File is full, we should open the conf folder and rename the file log4j.properties.template to log4j.properties and open it with wordpad for example.\n",
    "\n",
    "Inside the log4j.properties, we'll find the following line:\n",
    "```\n",
    "# Set everything to be logged to the console\n",
    "log4j.rootCategory=INFO, console\n",
    "```\n",
    "We will change it to:\n",
    "```\n",
    "# Set everything to be logged to the console\n",
    "log4j.rootCategory=ERROR, console\n",
    "```\n",
    "This change will remove all the logs that gets prints off when we'll run commands with Spark.\n",
    "Save the file and close it.\n",
    "\n",
    "### 2.4. winutils\n",
    "\n",
    "If you're not on windows you can skip this step, otherwise it's recommended.\n",
    "You need to download winutils, you can find it here (https://github.com/steveloughran/winutils)\n",
    "This is a windows binary, Hadoop requires native libraries on Windows to work properly -that includes accessing the file:// filesystem, where Hadoop uses some Windows APIs to implement posix-like file access permissions.\n",
    "\n",
    "This is implemented in hadoop.dll and winutils.exe.\n",
    "\n",
    "Now we need to create a winutil folder at the root of our Spark folder and we can call this new folder \"winutils\", inside it we'll create another folder called \"bin\" and copy our winutils.exe here.\n",
    "\n",
    "Open the command prompt (CMD) and type the following commands:\n",
    "```\n",
    "C:\\Users\\gabki>cd C:\\Program Files (x86)\\winutils\\bin\n",
    "\n",
    "C:\\Program Files (x86)\\winutils\\bin>dir\n",
    " Le volume dans le lecteur C s’appelle Windows-SSD\n",
    " Le numéro de série du volume est D801-AF53\n",
    "\n",
    " Répertoire de C:\\Program Files (x86)\\winutils\\bin\n",
    "\n",
    "14/12/2020  22:40    <DIR>          .\n",
    "14/12/2020  22:40    <DIR>          ..\n",
    "14/12/2020  22:27           108 032 winutils.exe\n",
    "               1 fichier(s)          108 032 octets\n",
    "               2 Rép(s)  16 753 942 528 octets libres\n",
    "C:\\Program Files (x86)\\winutils\\bin>mkdir \\tmp\\hive\n",
    "C:\\Program Files (x86)\\winutils\\bin>winutils.exe ls \\tmp\\hive\n",
    "d--------- 1 DESKTOP-2KDT29C\\gabki DESKTOP-2KDT29C\\gabki 0 Dec 14 2020 \\tmp\\hive\n",
    "C:\\Program Files (x86)\\winutils\\bin>dir \\tmp\\hive\n",
    " Le volume dans le lecteur C s’appelle Windows-SSD\n",
    " Le numéro de série du volume est D801-AF53\n",
    "\n",
    " Répertoire de C:\\tmp\\hive\n",
    "\n",
    "14/12/2020  22:57    <DIR>          .\n",
    "14/12/2020  22:57    <DIR>          ..\n",
    "               0 fichier(s)                0 octets\n",
    "               2 Rép(s)  17 155 751 936 octets libres\n",
    "C:\\Program Files (x86)\\winutils\\bin>set path=%path%;\"C:\\Program Files (x86)\\winutils\\bin\"\n",
    "C:\\Program Files (x86)\\winutils\\bin>winutils.exe ls \\tmp\\hive\n",
    "drwxrwxrwx 1 DESKTOP-2KDT29C\\gabki DESKTOP-2KDT29C\\gabki 0 Dec 14 2020 \\tmp\\hive\n",
    "```\n",
    "\n",
    "### 2.5. Definition of the environment variables\n",
    "\n",
    "We need to define the environment variables and we'll be ready to use Spark!\n",
    "\n",
    "Let's define the SPARK_HOME variable with the path to our Spark folder;\n",
    "JAVA_HOME with the path to the java JDK\n",
    "HADOOP_HOME with the path to the winutils\n",
    "\n",
    "### 2.6. Check your installation\n",
    "\n",
    "Now that everything is setup you can test spark to see if it runs!\n",
    "```\n",
    "C:\\Users\\gabriel>cd c:\\Spark\\bin\n",
    "c:\\Spark\\bin>pyspark\n",
    "Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020 15:52:53)\n",
    "SparkSession available as 'spark'.\n",
    "```\n",
    "We can also test the following programs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#df = spark.sql(\"select 'spark' as hello \")\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Query Requests\n",
    "\n",
    "First we need to initialize our SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row, SQLContext, SparkSession\n",
    "\n",
    "sc = SparkContext().getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Creating a Dataframe\n",
    "\n",
    "Let's create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+\n",
      "|studentID|studentName|      city|\n",
      "+---------+-----------+----------+\n",
      "|        1|    Gabriel|     Paris|\n",
      "|        2|  Stanislas|Montpelier|\n",
      "+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from array import array\n",
    "data = [(1, \"Gabriel\", \"Paris\"),(2, \"Stanislas\", \"Montpelier\")]\n",
    "#dataSchema = StructType(array(\n",
    "#    StructField(\"studentID\", IntegerType(), False),\n",
    "#    StructField(\"studentName\", StringType(), True),\n",
    "#    StructField(\"city\", StringType(), True)))\n",
    "rdd = sc.parallelize(data)\n",
    "students = rdd.map(lambda x: Row(studentID=int(x[0]), studentName=x[1], city=x[2]))\n",
    "table = sqlContext.createDataFrame(students) #, dataSchema\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Show the datatype\n",
    "We can see the datatypes of each column with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- studentID: long (nullable = true)\n",
      " |-- studentName: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Add a column\n",
    "You can add a column with the function withColumns which edit an existing column or creates it if it doesn't exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+-----------+\n",
      "|studentID|studentName|      city|pocketMoney|\n",
      "+---------+-----------+----------+-----------+\n",
      "|        1|    Gabriel|     Paris|         50|\n",
      "|        2|  Stanislas|Montpelier|         50|\n",
      "+---------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "table = table.withColumn(\"pocketMoney\", lit(50))\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Drop a column\n",
    "You can drop a column from the dataframe using the drop operation, we will drop our newly created column 'pocketMoney'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+\n",
      "|studentID|studentName|      city|\n",
      "+---------+-----------+----------+\n",
      "|        1|    Gabriel|     Paris|\n",
      "|        2|  Stanislas|Montpelier|\n",
      "+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = table.drop(\"pocketMoney\")\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Count the number of rows and colums\n",
    "You can count the number of rows using the count method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of columns and display their names using the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, ['studentID', 'studentName', 'city'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table.columns), table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Show a summary of the dataset\n",
    "\n",
    "This will display the mean, standard deviation, min, max, count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------+----------+\n",
      "|summary|         studentID|studentName|      city|\n",
      "+-------+------------------+-----------+----------+\n",
      "|  count|                 2|          2|         2|\n",
      "|   mean|               1.5|       null|      null|\n",
      "| stddev|0.7071067811865476|       null|      null|\n",
      "|    min|                 1|    Gabriel|Montpelier|\n",
      "|    max|                 2|  Stanislas|     Paris|\n",
      "+-------+------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Update data in a row\n",
    "We will use the filter method to select the row we want to modify and specify the column of the value to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------+\n",
      "|studentID|studentName|     city|\n",
      "+---------+-----------+---------+\n",
      "|        1|    Gabriel|    Paris|\n",
      "|        2|  Stanislas|Villejuif|\n",
      "+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "table = table.withColumn(\"city\",\n",
    "                            when(table[\"studentID\"] == 2,\"Villejuif\").otherwise(table[\"city\"]))\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. SQL Context\n",
    "\n",
    "We can also do some query operation using the SQL context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     city|\n",
      "+---------+\n",
      "|Villejuif|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.createOrReplaceTempView(\"table\")\n",
    "sqlContext.sql(\"SELECT city FROM table where studentID>=2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dataset Manipulation\n",
    "### 4.1. We first need to import a dataset\n",
    "Spark as an automatic formater for csv , json, and text we will use a csv with countries characteristics here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.option(\"header\",True).option(\"delimiter\",\";\").option(\"inferSchema\",True).csv(\"factbook.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Area(sq km): integer (nullable = true)\n",
      " |-- Birth rate(births/1000 population): double (nullable = true)\n",
      " |-- Current account balance: long (nullable = true)\n",
      " |-- Death rate(deaths/1000 population): double (nullable = true)\n",
      " |-- Debt - external: long (nullable = true)\n",
      " |-- Electricity - consumption(kWh): long (nullable = true)\n",
      " |-- Electricity - production(kWh): long (nullable = true)\n",
      " |-- Exports: long (nullable = true)\n",
      " |-- GDP: long (nullable = true)\n",
      " |-- GDP - per capita: integer (nullable = true)\n",
      " |-- GDP - real growth rate(%): double (nullable = true)\n",
      " |-- HIV/AIDS - adult prevalence rate(%): double (nullable = true)\n",
      " |-- HIV/AIDS - deaths: integer (nullable = true)\n",
      " |-- HIV/AIDS - people living with HIV/AIDS: integer (nullable = true)\n",
      " |-- Highways(km): integer (nullable = true)\n",
      " |-- Imports: long (nullable = true)\n",
      " |-- Industrial production growth rate(%): double (nullable = true)\n",
      " |-- Infant mortality rate(deaths/1000 live births): double (nullable = true)\n",
      " |-- Inflation rate (consumer prices)(%): double (nullable = true)\n",
      " |-- Internet hosts: integer (nullable = true)\n",
      " |-- Internet users: integer (nullable = true)\n",
      " |-- Investment (gross fixed)(% of GDP): double (nullable = true)\n",
      " |-- Labor force: integer (nullable = true)\n",
      " |-- Life expectancy at birth(years): double (nullable = true)\n",
      " |-- Military expenditures - dollar figure: long (nullable = true)\n",
      " |-- Military expenditures - percent of GDP(%): double (nullable = true)\n",
      " |-- Natural gas - consumption(cu m): long (nullable = true)\n",
      " |-- Natural gas - exports(cu m): long (nullable = true)\n",
      " |-- Natural gas - imports(cu m): long (nullable = true)\n",
      " |-- Natural gas - production(cu m): long (nullable = true)\n",
      " |-- Natural gas - proved reserves(cu m): long (nullable = true)\n",
      " |-- Oil - consumption(bbl/day): integer (nullable = true)\n",
      " |-- Oil - exports(bbl/day): integer (nullable = true)\n",
      " |-- Oil - imports(bbl/day): integer (nullable = true)\n",
      " |-- Oil - production(bbl/day): integer (nullable = true)\n",
      " |-- Oil - proved reserves(bbl): long (nullable = true)\n",
      " |-- Population: integer (nullable = true)\n",
      " |-- Public debt(% of GDP): double (nullable = true)\n",
      " |-- Railways(km): integer (nullable = true)\n",
      " |-- Reserves of foreign exchange & gold: long (nullable = true)\n",
      " |-- Telephones - main lines in use: integer (nullable = true)\n",
      " |-- Telephones - mobile cellular: integer (nullable = true)\n",
      " |-- Total fertility rate(children born/woman): double (nullable = true)\n",
      " |-- Unemployment rate(%): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             Country|\n",
      "+--------------------+\n",
      "|         Afghanistan|\n",
      "|            Akrotiri|\n",
      "|             Albania|\n",
      "|             Algeria|\n",
      "|      American Samoa|\n",
      "|             Andorra|\n",
      "|              Angola|\n",
      "|            Anguilla|\n",
      "|          Antarctica|\n",
      "| Antigua and Barbuda|\n",
      "|           Argentina|\n",
      "|             Armenia|\n",
      "|               Aruba|\n",
      "|Ashmore and Carti...|\n",
      "|           Australia|\n",
      "|             Austria|\n",
      "|          Azerbaijan|\n",
      "|         Bahamas The|\n",
      "|             Bahrain|\n",
      "|        Baker Island|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"Country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the import worked properly and we now have a dataset on which we can do querries \n",
    "\n",
    "We will try to do some simple as well as more complex querries to see how efficient spark is.\n",
    "\n",
    "### 4.2. Querries examples\n",
    "\n",
    "#### 4.2.1. Where clause example\n",
    "Using dsl querries we can select the countries where the deat rate is superior to 20. Meaning the countries where more than 20 people our of 1000 dies a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------------+\n",
      "|             Country|Death rate(deaths/1000 population)|\n",
      "+--------------------+----------------------------------+\n",
      "|         Afghanistan|                             20.75|\n",
      "|              Angola|                              25.9|\n",
      "|            Botswana|                             29.36|\n",
      "|Central African R...|                             20.27|\n",
      "|             Lesotho|                             25.03|\n",
      "|              Malawi|                             23.39|\n",
      "|          Mozambique|                             20.99|\n",
      "|               Niger|                             21.33|\n",
      "|        Sierra Leone|                             20.61|\n",
      "|        South Africa|                             21.32|\n",
      "|           Swaziland|                             25.26|\n",
      "|              Zambia|                             20.23|\n",
      "|            Zimbabwe|                             24.66|\n",
      "+--------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dataset.select(\"Country\",\"Death rate(deaths/1000 population)\").where(col(\"Death rate(deaths/1000 population)\")>20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Max Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|max(Area(sq km))|\n",
      "+----------------+\n",
      "|        17075200|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.agg({\"Area(sq km)\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Order By example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+\n",
      "|          Country|          GDP|\n",
      "+-----------------+-------------+\n",
      "|             Iraq|  89800000000|\n",
      "|             Chad|  15660000000|\n",
      "|          Liberia|   2903000000|\n",
      "|Equatorial Guinea|   1270000000|\n",
      "|        Venezuela| 145200000000|\n",
      "|            Macau|   9100000000|\n",
      "|          Ukraine| 299100000000|\n",
      "|           Angola|  23170000000|\n",
      "|         Ethiopia|  54890000000|\n",
      "|    Liechtenstein|    825000000|\n",
      "|         Mongolia|   5332000000|\n",
      "|       Tajikistan|   7950000000|\n",
      "|          Uruguay|  49270000000|\n",
      "|    Faroe Islands|   1000000000|\n",
      "|       Azerbaijan|  30010000000|\n",
      "|          Georgia|  14450000000|\n",
      "|       Kazakhstan| 118400000000|\n",
      "|            China|7262000000000|\n",
      "|          Armenia|  13650000000|\n",
      "|            Qatar|  19490000000|\n",
      "+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc,desc\n",
    "dataset.select(\"Country\",\"GDP\").orderBy(col(\"GDP - real growth rate(%)\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the Highest GDP using the collect method in this command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|           GDP|      Country|\n",
      "+--------------+-------------+\n",
      "|11750000000000|United States|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"GDP\",\"Country\").orderBy(col(\"GDP\").desc()).limit(1).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4. Nested querry example\n",
    "We can find the country with the second highest gdp using a nested querry like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|        Country|           GDP|\n",
      "+---------------+--------------+\n",
      "|#European Union|11650000000000|\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select(\"Country\",\"GDP\").where(col(\"GDP\") <   \\\n",
    "                                      ( dataset.select(\"GDP\").orderBy(col(\"GDP\").desc()).limit(1).collect()[0][\"GDP\"] ) \\\n",
    "                                      ).orderBy(col(\"GDP\").desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5. Describe \n",
    "The describe function allows to process the count, mean, standard deviation, min & max of the columns selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+------------------+\n",
      "|summary|                 GDP|      Area(sq km)|      Railways(km)|\n",
      "+-------+--------------------+-----------------+------------------+\n",
      "|  count|                 230|              263|               134|\n",
      "|   mean|2.925613713043478...|584987.4866920152|10217.246268656716|\n",
      "| stddev|1.252331334754941...|1881415.546777828|29857.440213148762|\n",
      "|    min|             1500000|                0|                 6|\n",
      "|    max|      11750000000000|         17075200|            228464|\n",
      "+-------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.describe(\"GDP\",\"Area(sq km)\",\"Railways(km)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparison with other technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "From what we saw Spark support sql and dsl querries. We can do everything SQL does and even more using pySpark or other modules, allowing to use others function under a python environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
